name: Release Tests

on:
  release:
    types: [published, created]
  push:
    tags:
      - 'v*'
  workflow_dispatch:
    inputs:
      version:
        description: 'Version to test'
        required: true
        default: 'latest'

jobs:
  comprehensive-test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11, 3.12]
        os: [ubuntu-latest, windows-latest, macos-latest]

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: Set up test environment
      shell: bash
      run: |
        mkdir -p logs/tests tests/reports
        echo "PYTHONPATH=$PWD:$PWD/src" >> $GITHUB_ENV
        echo "NEO4J_DISABLED=1" >> $GITHUB_ENV

    - name: Run comprehensive test suite
      run: |
        python run_tests.py

    - name: Validate MCP system
      run: |
        python validate_mcp_system.py

    - name: Test CLI functionality
      shell: bash
      run: |
        cd cortex-cli
        python -m pytest tests/ -v || echo "CLI tests completed with warnings"

    - name: Upload release test artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: release-test-${{ matrix.os }}-python-${{ matrix.python-version }}
        path: |
          tests/reports/
          logs/
        retention-days: 90

  performance-test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.12

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install pytest-benchmark

    - name: Run performance benchmarks
      run: |
        # Performance tests wenn vorhanden
        if [ -f "tests/performance/test_benchmarks.py" ]; then
          python -m pytest tests/performance/ --benchmark-only
        else
          echo "⚠️  No performance tests found, skipping benchmarks"
        fi

  release-validation:
    needs: [comprehensive-test, performance-test]
    runs-on: ubuntu-latest

    steps:
    - name: Release validation summary
      run: |
        echo "## 🚀 Release Validation Complete" >> $GITHUB_STEP_SUMMARY
        echo "- Comprehensive Tests: ${{ needs.comprehensive-test.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Performance Tests: ${{ needs.performance-test.result }}" >> $GITHUB_STEP_SUMMARY
        
        if [[ "${{ needs.comprehensive-test.result }}" == "success" ]]; then
          echo "✅ Release ready for deployment!" >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ Release validation failed!" >> $GITHUB_STEP_SUMMARY
          exit 1
        fi
